# セットアップ手順

## 必要な環境
- Python 3.8以上
- Ollama（指定パス: `C:\Users\e9uch\AppData\Local\Programs\Ollama\ollama.exe`）

## インストール手順

1. 依存パッケージのインストール
```bash
pip install -r requirements.txt
```

2. Ollamaの起動確認
Ollamaが起動していることを確認してください。Ollamaが起動していない場合は、以下のコマンドで起動できます：
```bash
"C:\Users\e9uch\AppData\Local\Programs\Ollama\ollama.exe" serve
```

3. アプリケーションの起動
```bash
python run.py
```

**注意**: 初回起動時、`tinyllama`モデル（約637MB）が自動的にダウンロードされます。ダウンロードには数分かかる場合があります。

4. ブラウザでアクセス
http://localhost:5000 にアクセスしてください

## 使用方法

### 機能の切り替え
上部のメニューバーから以下の4つの機能を切り替えることができます：
- **チャットボット**: 社内文書を参照しながら質問に答えます
- **日報生成**: 音声入力や設備データを日報フォーマットに変換します
- **異常検知**: マルチモーダルな入力データから異常を検知します
- **生産計画**: 様々な情報から動的な生産計画を生成します

### 画面構成
- **左側パネル**: ドキュメント一覧とファイルアップロード
- **中央パネル**: チャットインターフェース
- **右側パネル**: 回答の根拠・参照元を表示

## よくある質問

### Q: モデルは毎回インストールすることになりますか？
**A: いいえ、一度ダウンロードされれば次回以降は自動的に使用されます。**
- アプリ起動時にモデルの存在を確認し、既にインストールされている場合はダウンロードをスキップします
- モデルは`%USERPROFILE%\.ollama\models`に保存されます

### Q: モデルはローカル完結の物ですか？情報漏洩の心配はありませんか？
**A: はい、完全にローカル完結です。データは一切外部に送信されません。**
- Ollamaはローカルで動作するLLMエンジンです
- すべての処理（モデル実行、ドキュメント検索）はローカルマシン上で完結します
- インターネット接続は初回のモデルダウンロード時のみ必要です
- ドキュメントや質問内容は外部サーバーに送信されることはありません
- 社内の機密情報を安全に扱えます

### Q: 右側の根拠表示はちゃんと機能していますか？
**A: はい、ドキュメント検索機能を実装しています。**
- チャットボット機能では、質問に関連するドキュメントを自動的に検索します
- 検索結果は右側パネルに「根拠・参照元」として表示されます
- ドキュメントの該当部分が表示され、どのドキュメントの何行目を参照したかが分かります
- 現在は`docs`フォルダ内の`.txt`と`.md`ファイルを検索対象としています

## 注意事項

- デモアプリのため、一部の機能は簡易実装になっています
- Ollamaのモデル名はデフォルトで`tinyllama`（軽量モデル、約637MB）に設定されています。利用可能なモデルに変更する場合は`backend/ollama_client.py`の`self.model`を編集してください
- 初回起動時にモデルが自動的にダウンロードされます（約637MB、数分かかる場合があります）
- 実際のOllamaモデルが利用できない場合は、デモ用のレスポンスが返されます

